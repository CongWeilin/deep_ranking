{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import NDCG\n",
    "from load_mslr import get_time, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_structure, sigma=1.0):\n",
    "        super(Model, self).__init__()\n",
    "        modules = []\n",
    "        for i in range(len(model_structure) - 1):\n",
    "            modules.append(nn.Linear(model_structure[i], model_structure[i+1]))\n",
    "            modules.append(nn.LeakyReLU())\n",
    "        modules.append(nn.Linear(model_structure[-1], 1))\n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.sigma = sigma\n",
    "        self.activation = nn.ReLU6()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(inputs)\n",
    "        outputs = self.activation(outputs)*self.sigma\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = torch.float32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr=0.0001\n",
    "\n",
    "model_structure = [136, 64, 16]\n",
    "sigma = 1.0\n",
    "model = Model(model_structure, sigma).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.75) # decrease lr while training\n",
    "\n",
    "ndcg_gain_in_train = 'exp2' # or 'identity'\n",
    "ideal_dcg = NDCG(2**9, ndcg_gain_in_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-21 13:10:55 load from pickle file data/mslr-web10k/train.pkl\n",
      "2020-04-21 13:10:55 load from pickle file data/mslr-web10k/vali.pkl\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_file, valid_file = \"train.txt\", \"vali.txt\"\n",
    "data_dir = 'data/mslr-web10k/'\n",
    "\n",
    "train_data = os.path.join(data_dir, train_file)\n",
    "train_loader = DataLoader(train_data)\n",
    "df_train = train_loader.load()\n",
    "\n",
    "valid_data = os.path.join(data_dir, valid_file)\n",
    "valid_loader = DataLoader(valid_data)\n",
    "df_valid = valid_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cross_entropy_loss(model, device, loader, epoch, sigma=1.0):\n",
    "    \"\"\"\n",
    "    formula in https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf\n",
    "\n",
    "    C = 0.5 * (1 - S_ij) * sigma * (si - sj) + log(1 + exp(-sigma * (si - sj)))\n",
    "    when S_ij = 1:  C = log(1 + exp(-sigma(si - sj)))\n",
    "    when S_ij = -1: C = log(1 + exp(-sigma(sj - si)))\n",
    "    sigma can change the shape of the curve\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        total_cost = 0\n",
    "        total_pairs = loader.get_num_pairs()\n",
    "        pairs_in_compute = 0\n",
    "        for X, Y in loader.generate_batch_per_query(loader.df):\n",
    "            Y = Y.reshape(-1, 1)\n",
    "            rel_diff = Y - Y.T\n",
    "            pos_pairs = (rel_diff > 0).astype(np.float32)\n",
    "            num_pos_pairs = np.sum(pos_pairs, (0, 1))\n",
    "            # skip negative sessions, no relevant info:\n",
    "            if num_pos_pairs == 0:\n",
    "                continue\n",
    "            neg_pairs = (rel_diff < 0).astype(np.float32)\n",
    "            num_pairs = 2 * num_pos_pairs  # num pos pairs and neg pairs are always the same\n",
    "            pos_pairs = torch.tensor(pos_pairs, device=device)\n",
    "            neg_pairs = torch.tensor(neg_pairs, device=device)\n",
    "            Sij = pos_pairs - neg_pairs\n",
    "            # only calculate the different pairs\n",
    "            diff_pairs = pos_pairs + neg_pairs\n",
    "            pairs_in_compute += num_pairs\n",
    "\n",
    "            X_tensor = torch.Tensor(X).to(device)\n",
    "            y_pred = model(X_tensor)\n",
    "            y_pred_diff = y_pred - y_pred.t()\n",
    "\n",
    "            # logsigmoid(x) = log(1 / (1 + exp(-x))) equivalent to log(1 + exp(-x))\n",
    "            C = 0.5 * (1 - Sij) * sigma * y_pred_diff - F.logsigmoid(-sigma * y_pred_diff)\n",
    "            C = C * diff_pairs\n",
    "            cost = torch.sum(C, (0, 1))\n",
    "            if cost.item() == float('inf') or np.isnan(cost.item()):\n",
    "                import ipdb; ipdb.set_trace()\n",
    "            total_cost += cost\n",
    "\n",
    "        assert total_pairs == pairs_in_compute\n",
    "        avg_cost = total_cost / total_pairs\n",
    "    print(\"Epoch {}: pairwise corss entropy loss {:.6f}, total_paris {}\".format(\n",
    "            epoch, avg_cost.item(), total_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ndcg_at_k(inference_model, device, df_valid, valid_loader, batch_size, k_list, epoch):\n",
    "    print(\"Eval Phase evaluate NDCG @ {}\".format(k_list))\n",
    "    ndcg_metrics = {k: NDCG(k) for k in k_list}\n",
    "    qids, rels, scores = [], [], []\n",
    "    inference_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for qid, rel, x in valid_loader.generate_query_batch(df_valid, batch_size):\n",
    "            if x is None or x.shape[0] == 0:\n",
    "                continue\n",
    "            y_tensor = inference_model.forward(torch.Tensor(x).to(device))\n",
    "            scores.append(y_tensor.cpu().numpy().squeeze())\n",
    "            qids.append(qid)\n",
    "            rels.append(rel)\n",
    "\n",
    "    qids = np.hstack(qids)\n",
    "    rels = np.hstack(rels)\n",
    "    scores = np.hstack(scores)\n",
    "    result_df = pd.DataFrame({'qid': qids, 'rel': rels, 'score': scores})\n",
    "    session_ndcgs = defaultdict(list)\n",
    "    for qid in result_df.qid.unique():\n",
    "        result_qid = result_df[result_df.qid == qid].sort_values('score', ascending=False)\n",
    "        rel_rank = result_qid.rel.values\n",
    "        for k, ndcg in ndcg_metrics.items():\n",
    "            if ndcg.maxDCG(rel_rank) == 0:\n",
    "                continue\n",
    "            ndcg_k = ndcg.evaluate(rel_rank)\n",
    "            if not np.isnan(ndcg_k):\n",
    "                session_ndcgs[k].append(ndcg_k)\n",
    "\n",
    "    ndcg_result = {k: np.mean(session_ndcgs[k]) for k in k_list}\n",
    "    ndcg_result_print = \", \".join([\"NDCG@{}: {:.5f}\".format(k, ndcg_result[k]) for k in k_list])\n",
    "    print(\"evaluate {}\".format(ndcg_result_print))\n",
    "    return ndcg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "X, Y = next(valid_loader.generate_batch_per_query(df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_solver(pairwise_scores, ofe_score_min_cap=0, ofe_score_max_cap=1):\n",
    "    # num_docs = pairwise_scores.shape[0]\n",
    "    ofe_tree_sum_clipped = pairwise_scores.clip(ofe_score_min_cap, ofe_score_max_cap)\n",
    "    ofe_tree_sum_norm = 6 * (ofe_tree_sum_clipped-ofe_score_min_cap)/(ofe_score_max_cap-ofe_score_min_cap) - 3\n",
    "    ofe_score = 1/(1+np.exp(-ofe_tree_sum_norm))\n",
    "    ofe_score_diag = ofe_score.copy()\n",
    "    np.fill_diagonal(ofe_score_diag, 0.5)\n",
    "    scores = ofe_score_diag.sum(axis=1)/10\n",
    "    return scores, ofe_score_diag, ofe_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.25350825, 0.24352709, 0.16855005, 0.2261107 , 0.25504928]),\n",
       " array([[0.5       , 0.69938651, 0.582792  , 0.30517379, 0.44773022],\n",
       "        [0.85577158, 0.5       , 0.157737  , 0.40811404, 0.5136483 ],\n",
       "        [0.70748361, 0.04809095, 0.5       , 0.06596003, 0.36396593],\n",
       "        [0.24138335, 0.89969618, 0.2447741 , 0.5       , 0.37525341],\n",
       "        [0.46686904, 0.15403587, 0.59950058, 0.83008734, 0.5       ]]),\n",
       " array([[0.5       , 0.69938651, 0.582792  , 0.30517379, 0.44773022],\n",
       "        [0.85577158, 0.5       , 0.157737  , 0.40811404, 0.5136483 ],\n",
       "        [0.70748361, 0.04809095, 0.5       , 0.06596003, 0.36396593],\n",
       "        [0.24138335, 0.89969618, 0.2447741 , 0.5       , 0.37525341],\n",
       "        [0.46686904, 0.15403587, 0.59950058, 0.83008734, 0.5       ]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(5,5)\n",
    "np.fill_diagonal(a, 0.5)\n",
    "apply_solver(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     model.train()\n",
    "#     model.zero_grad()\n",
    "\n",
    "#     count = 0\n",
    "#     batch_size = 200\n",
    "#     grad_batch, y_pred_batch = [], []\n",
    "\n",
    "#     for X, Y in train_loader.generate_batch_per_query():\n",
    "#         if np.sum(Y) == 0:\n",
    "#             # negative session, cannot learn useful signal\n",
    "#             continue\n",
    "#         N = 1.0 / ideal_dcg.maxDCG(Y)\n",
    "\n",
    "#         X_tensor = torch.tensor(X, dtype=precision, device=device)\n",
    "#         y_pred = model(X_tensor)\n",
    "#         y_pred_batch.append(y_pred)\n",
    "#         # compute the rank order of each document\n",
    "#         rank_df = pd.DataFrame({\"Y\": Y, \"doc\": np.arange(Y.shape[0])})\n",
    "#         rank_df = rank_df.sort_values(\"Y\").reset_index(drop=True)\n",
    "#         rank_order = rank_df.sort_values(\"doc\").index.values + 1\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             pos_pairs_score_diff = 1.0 + torch.exp(sigma * (y_pred - y_pred.t()))\n",
    "\n",
    "#             Y_tensor = torch.tensor(Y, dtype=precision, device=device).view(-1, 1)\n",
    "#             rel_diff = Y_tensor - Y_tensor.t()\n",
    "#             pos_pairs = (rel_diff > 0).type(precision)\n",
    "#             neg_pairs = (rel_diff < 0).type(precision)\n",
    "#             Sij = pos_pairs - neg_pairs\n",
    "#             if ndcg_gain_in_train == \"exp2\":\n",
    "#                 gain_diff = torch.pow(2.0, Y_tensor) - torch.pow(2.0, Y_tensor.t())\n",
    "#             elif ndcg_gain_in_train == \"identity\":\n",
    "#                 gain_diff = Y_tensor - Y_tensor.t()\n",
    "\n",
    "#             rank_order_tensor = torch.tensor(rank_order, dtype=precision, device=device).view(-1, 1)\n",
    "#             decay_diff = 1.0 / torch.log2(rank_order_tensor + 1.0) - 1.0 / torch.log2(rank_order_tensor.t() + 1.0)\n",
    "\n",
    "#             delta_ndcg = torch.abs(N * gain_diff * decay_diff)\n",
    "#             lambda_update = sigma * (0.5 * (1 - Sij) - 1 / pos_pairs_score_diff) * delta_ndcg\n",
    "#             lambda_update = torch.sum(lambda_update, 1, keepdim=True)\n",
    "\n",
    "#             assert lambda_update.shape == y_pred.shape\n",
    "#             grad_batch.append(lambda_update)\n",
    "\n",
    "#         count += 1\n",
    "#         if count % batch_size == 0:\n",
    "#             for grad, y_pred in zip(grad_batch, y_pred_batch):\n",
    "#                 y_pred.backward(grad / batch_size)\n",
    "\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "#             grad_batch, y_pred_batch = [], []  # grad_batch, y_pred_batch used for gradient_acc\n",
    "            \n",
    "#             # loss = F.mse_loss(y_pred, Y_tensor).item()\n",
    "#             # print('queries: ', count, '| loss: ', loss, ' (not a good metric)')\n",
    "            \n",
    "#     if i % 5 == 0: # validate model every 5 epoch\n",
    "#         print(get_time(), \"eval for epoch: {}\".format(i))\n",
    "#         eval_cross_entropy_loss(model, device, valid_loader, i)\n",
    "#         eval_ndcg_at_k(model, device, df_valid, valid_loader, 100000, [10, 30], i)\n",
    "\n",
    "#     # optimizer.step()\n",
    "#     print(get_time(), \"training dataset at epoch {}, total queries: {}\".format(i, count))\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_time(), \"eval for epoch: {}\".format(i))\n",
    "# eval_cross_entropy_loss(model, device, valid_loader, i)\n",
    "# eval_ndcg_at_k(model, device, df_valid, valid_loader, 100000, [10, 30], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model\n",
    "# torch.save(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
